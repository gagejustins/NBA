
The data from this report was collected from all 30 NBA teams over the course of the 2015-2016 season. The independent (predicting) variable is defensive rating, which according to the data source is “an estimate of points allowed per 100 possessions.”  That means that the lower the rating, the more effective the defense is. The dependent variables is the amount of wins the team had (out of 82 total games) in the season. The data is from a site called Basketball Reference, and can be accessed here: http://www.basketball-reference.com/leagues/NBA_2016_ratings.html. 

If the tables / plots don't appear, try downloading the notebook and running it in R Studio.

First, let's install our key packages.

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
```

Load our dataframe of NBA 2015-2016 statistics.

```{r}
df = tbl_df(read.csv("~/Desktop/NYU_Fall_2016/Regressions\ and\ Multivariate\ Data\ Analysis/NBA/teams.csv"))
```

Let's take a look at it with the str() command.

```{r}
str(df)
```

Let's take a look at our two variables of interest: Wins (W) and Defensive Rating (DRtg).

```{r}
ggplot(df, aes(DRtg, W)) + geom_point() + ggtitle("Defensive Ratings vs. Wins") + labs(x = "Defensive Ratings") + labs(y = "Wins")
```

Hm...There does appear to be a strong negative correlation between Defensive Rating (lower = better) and Wins.

Time to make a linear regression model! Let's start using all of the data, and see if there's a relationship between Wins (W) and Defensive Rating (DRtg).

```{r}
regDef <- lm(W ~ DRtg, data=df)
summary(regDef)
```

Here's a plot of the regression line.

```{r}
regplot <- ggplot(df, aes(DRtg, W)) + geom_point()
regplot <- regplot + geom_smooth(method="lm")
regplot
```


Now let's plot the residuals.

First, we'll plot the residuals vs. fitted values.

```{r}
pres <- ggplot(regDef, aes(regDef$fitted.values, regDef$residuals)) + geom_point()
pres <- pres + geom_hline(yintercept = 0, col = "red", linetype = "dashed")
pres <- pres + ggtitle("Residuals vs. Fitted") + labs(x = "Fitted Values") + labs(y = "Residuals")
pres
```

The residuals look evenly random and follow no pattern, suggesting homoscedasticity.

Here's a histogram of the residuals.

```{r}
phist <- ggplot(regDef, aes(regDef$residuals))+ geom_histogram(fill = "blue")
phist <- phist + ggtitle("Residual Frequencies") + labs(x = "Residual") + labs(y = "Count")
phist
```

Again, they appear to be normally distributed. Finally, here's a normal probability plot.

```{r}
qqnorm(regDef$residuals)
```

Indeed, the residuals from our model seem to follow a pretty clear line, suggesting they're normally distributed. 

From the residual graphs though, we can identify two outliers that may be skewing our data - the Golden State Warriors (average defensive rating, most wins in the league) and the Philadelphia 76ers (above average defensive rating, least wins in the league).

Let's remove these teams from our dataframe and model. We'll start with the Warriors.

```{r}
newdf <- filter(df, Team != "Golden State Warriors")
```

Now let's plot the data again.

```{r}
pnew <- ggplot(newdf, aes(DRtg, W)) + geom_point()
pnew <- pnew + ggtitle("Defensive Rating vs. Wins") + labs(x = "Defensive Rating", y = "Wins")
pnew
```

Now we can plot a new regression and see how it compares to the original one.

```{r}
newRegDef <- lm(W ~ DRtg, data=newdf)
summary(newRegDef)
```

Too see how we've improved, let's subtract some of these new metrics from the old ones.

```{r}
print(paste("The difference in R^2 is:" , (summary(newRegDef)$r.squared) - summary(regDef)$r.squared))
print(paste("The difference in coefficients is:" , (newRegDef$coefficients[2] - regDef$coefficients[2])))
```

So the R^2 of the new regression is higher, and the new coefficient is lower (negative values).

Here's a plot of the new regression line.

```{r}
resplot <- ggplot(newdf, aes(DRtg, W)) + geom_point()
resplot <- resplot + geom_smooth(method="lm")
resplot <- resplot + ggtitle("OLS Regression") + labs(x = "Defensive Rating", y = "Wins")
resplot
```

We can see that the new regression line hits the leftmost point square in the middle now, while in the previous line it went clean over it.

Here are the residual plots.

```{r}
newresplot <- ggplot(newdf, aes(newRegDef$fitted.values, newRegDef$residuals)) + geom_point()
newresplot <- newresplot + geom_hline(yintercept = 0, col = "red", linetype = "dashed")
newresplot <- newresplot + ggtitle("Residuals") + labs(x = "Fitted Values", y = "Residuals")
newresplot
```

The upper outlier is gone, so our graph shifts down. Here's the histogram.

```{r}
newreshist <- ggplot(newdf, aes(newRegDef$residuals)) + geom_histogram(fill = "blue") 
newreshist <- newreshist + ggtitle("Residuals") + labs(x = "Residuals", y = "Count")
newreshist
```

Again, with the positive outlier removed, the graph shifts to the left. Here's the normal probability plot.

```{r}
qqnorm(newRegDef$residuals)
```

Like above, the uppermost point has been removed so the graph shifts downward.

Finally, let's remove the second outlier, the Philadelphia 76ers. To recall, they had an above average defensive rating, but the least wins in the league.

```{r}
finaldf <- filter(newdf, Team != "Philadelphia 76ers")
```

Here's the new plot of the data.

```{r}
finalplot <- ggplot(finaldf, aes(DRtg, W)) + geom_point()
finalplot <- finalplot + ggtitle("Defensive Rating vs. Wins") + labs(x = "Defensive Rating", y = "Wins")
finalplot
```

That looks much better. Now let's try the regression and compare it to the previous two.

```{r}
finalRegDef <- lm(data=finaldf, W ~ DRtg)
summary(finalRegDef)
```

```{r}
print(paste("The difference in R^2 from the original regression is: " , (summary(finalRegDef)$r.squared) - summary(regDef)$r.squared))

print(paste("The difference in R^2 from the last regression is: " , (summary(finalRegDef)$r.squared) - summary(newRegDef)$r.squared))

print(paste("The difference in coefficient from the original regression is: " , (finalRegDef$coefficients[2]) - regDef$coefficients[2]))

print(paste("The difference in coefficient from the last regression is: " , (finalRegDef$coefficients[2]) - newRegDef$coefficients[2]))
```

By removing the Warriors AND the 76ers, we've shaved almost .5 off of the coefficient, and upped the R^2 by 6%.

Here's the plotted regression.

```{r}
finalresplot <- ggplot(finaldf, aes(DRtg, W)) + geom_point()
finalresplot <- finalresplot + geom_smooth(method="lm")
finalresplot <- finalresplot + ggtitle("Regression") + labs(x = "Defensive Rating", y = "Wins")
finalresplot
```

The final regression line moves further below that leftmost point because of that lower coefficient / slope.

Here are the the residuals plots.

```{r}
finalresplot <- ggplot(finaldf, aes(finalRegDef$fitted.values, finalRegDef$residuals)) + geom_point()
finalresplot <- finalresplot + geom_hline(yintercept = 0, col = "red", linetype = "dashed")
finalresplot <- finalresplot + ggtitle("Residuals") + labs(x = "Fitted Values", y = "Residuals")
finalresplot
```

And the histogram.

```{r}
finalreshist <- ggplot(finaldf, aes(finalRegDef$residuals)) + geom_histogram(fill = "blue")
finalreshist <- finalreshist + ggtitle("Residuals") + labs(x = "Residuals", y = "Count")
finalreshist
```

Finally, we'll plot the normal probability distribution.

```{r}
qqnorm(finalRegDef$residuals)
```

With the bottom most outlier removed, the plot can be squeezed to a y = (-10,10) limit and definitely looks normally distributed.
